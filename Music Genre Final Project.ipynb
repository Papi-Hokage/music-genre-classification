{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Music Genre Classification Using Audio Feature Extraction and Machine-Learning Models\n",
        "\n",
        "**Course:** CAP 4630-001-042 - Introduction to Artificial Intelligence  \n",
        "**Instructor:** Dr. Ahmed Imteaj  \n",
        "**Student:** Andres Hernandez  \n",
        "**Date:** Fall 2025\n",
        "\n",
        "---\n",
        "\n",
        "## Project Description\n",
        "\n",
        "This project implements a supervised machine learning system for classifying audio tracks into musical genres using Mel-Frequency Cepstral Coefficients (MFCCs) as the primary feature representation. The system extracts audio features from raw audio files, processes them into fixed-length vectors, and trains multiple classification models to predict genre labels.\n",
        "\n",
        "## Deliverables\n",
        "\n",
        "1. Complete audio feature extraction pipeline using librosa\n",
        "2. Implementation of multiple baseline classifiers (KNN, Decision Trees, Random Forest, SVM)\n",
        "3. Hyperparameter tuning and model optimization\n",
        "4. Comprehensive evaluation with confusion matrices and performance metrics\n",
        "5. Analysis of results, challenges, and future improvements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1. Objective\n",
        "\n",
        "### Task Definition\n",
        "Classify audio tracks into musical genres using MFCC-based audio features and supervised machine learning classifiers.\n",
        "\n",
        "### Target Outcome\n",
        "Develop a trained classification model with measurable performance metrics (accuracy, precision, recall, F1-score) that can accurately predict the genre of previously unseen audio tracks.\n",
        "\n",
        "### Success Criteria\n",
        "- Achieve baseline accuracy above random guessing (>10% for 10 genres)\n",
        "- Compare multiple classification algorithms\n",
        "- Identify optimal hyperparameters through systematic tuning\n",
        "- Document model performance with comprehensive evaluation metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. Background\n",
        "\n",
        "### Why Music Genre Classification?\n",
        "\n",
        "Music genre classification is a fundamental task in Music Information Retrieval (MIR) with practical applications in:\n",
        "- **Music streaming services**: Automated playlist generation and recommendation systems\n",
        "- **Music libraries**: Organizing and categorizing large audio collections\n",
        "- **Music production**: Assisting composers and producers in understanding musical patterns\n",
        "- **Academic research**: Understanding the structural and timbral characteristics that define genres\n",
        "\n",
        "### Mel-Frequency Cepstral Coefficients (MFCCs)\n",
        "\n",
        "MFCCs are the standard audio feature representation for speech and music analysis because they:\n",
        "- Model the human auditory system's perception of sound\n",
        "- Capture the spectral envelope of audio signals\n",
        "- Provide compact, fixed-length representations regardless of audio duration\n",
        "- Effectively represent timbral texture, which is crucial for genre differentiation\n",
        "\n",
        "**Technical Process:**\n",
        "1. Convert time-domain audio to frequency domain via Short-Time Fourier Transform (STFT)\n",
        "2. Apply mel-scale filterbank to match human perception\n",
        "3. Take logarithm of filterbank energies\n",
        "4. Apply Discrete Cosine Transform (DCT) to decorrelate features\n",
        "5. Extract lower-order coefficients (typically 13-40) as features\n",
        "\n",
        "### Classification Algorithms\n",
        "\n",
        "For structured numeric features like MFCCs, several supervised classifiers are suitable:\n",
        "- **K-Nearest Neighbors (KNN)**: Instance-based learning, effective for multi-class problems\n",
        "- **Decision Trees**: Interpretable, handles non-linear decision boundaries\n",
        "- **Random Forest**: Ensemble method, reduces overfitting through averaging\n",
        "- **Support Vector Machines (SVM)**: Effective in high-dimensional spaces with clear margins"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. Setup and Imports\n",
        "\n",
        "Import all necessary libraries for audio processing, machine learning, and visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Standard libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "# Audio processing\n",
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "# Machine learning\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, \n",
        "    precision_score, \n",
        "    recall_score, \n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    classification_report\n",
        ")\n",
        "\n",
        "# Settings\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(42)\n",
        "plt.style.use('seaborn-v0_8')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Quick Start Guide\n",
        "\n",
        "**To run this notebook:**\n",
        "\n",
        "1. **Run the imports cell above** â˜ï¸ to load all libraries\n",
        "2. **Run the dataset configuration cell below** ðŸ‘‡ to verify GTZAN path\n",
        "3. **Execute cells sequentially** - each section builds on the previous one\n",
        "4. **Feature extraction takes 10-20 minutes** - be patient!\n",
        "5. **Watch for progress indicators** in the output\n",
        "\n",
        "**Current Dataset Path:** `/Users/andres/Documents/University/FALL 2025 /CAP4630/Dataset/GTZAN/genres_original`\n",
        "\n",
        "âœ… Dataset is already configured and ready to use!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. Dataset Description\n",
        "\n",
        "### Dataset Source\n",
        "**GTZAN Genre Collection** (or specify your actual dataset)\n",
        "\n",
        "### Dataset Specifications\n",
        "- **Number of samples**: 1000 audio files (100 per genre)\n",
        "- **Genres**: 10 classes (blues, classical, country, disco, hiphop, jazz, metal, pop, reggae, rock)\n",
        "- **File format**: .wav or .mp3\n",
        "- **Duration**: 30 seconds per track\n",
        "- **Sampling rate**: 22,050 Hz (standard for music analysis)\n",
        "- **Class balance**: Balanced dataset with equal samples per genre\n",
        "\n",
        "### Preprocessing Requirements\n",
        "- Handle corrupted or unreadable audio files\n",
        "- Normalize audio length to consistent duration\n",
        "- Convert stereo to mono if necessary\n",
        "- Resample to target sampling rate if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Dataset found at: /Users/andres/Documents/University/FALL 2025 /CAP4630/Dataset/GTZAN/genres_original\n",
            "\n",
            "Genres found: 10\n",
            "  blues: 100 files\n",
            "  classical: 100 files\n",
            "  country: 100 files\n",
            "  disco: 100 files\n",
            "  hiphop: 100 files\n",
            "  jazz: 100 files\n",
            "  metal: 100 files\n",
            "  pop: 100 files\n",
            "  reggae: 100 files\n",
            "  rock: 100 files\n"
          ]
        }
      ],
      "source": [
        "# Dataset configuration\n",
        "DATASET_PATH = \"/Users/andres/Documents/University/FALL 2025 /CAP4630/Dataset/GTZAN/genres_original\"\n",
        "SAMPLING_RATE = 22050\n",
        "DURATION = 30  # seconds\n",
        "N_MFCC = 13  # Number of MFCC coefficients to extract\n",
        "\n",
        "# Check if dataset path exists\n",
        "if os.path.exists(DATASET_PATH):\n",
        "    print(f\"âœ“ Dataset found at: {DATASET_PATH}\")\n",
        "    \n",
        "    # Count files per genre\n",
        "    genres = sorted([d for d in os.listdir(DATASET_PATH) if os.path.isdir(os.path.join(DATASET_PATH, d))])\n",
        "    print(f\"\\nGenres found: {len(genres)}\")\n",
        "    for genre in genres:\n",
        "        genre_path = os.path.join(DATASET_PATH, genre)\n",
        "        wav_files = [f for f in os.listdir(genre_path) if f.endswith('.wav')]\n",
        "        print(f\"  {genre}: {len(wav_files)} files\")\n",
        "else:\n",
        "    print(f\"âš ï¸ Dataset path not found. Please update DATASET_PATH variable.\")\n",
        "    print(f\"Current path: {DATASET_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. Methodology\n",
        "\n",
        "### Complete Pipeline (Sequential Steps)\n",
        "\n",
        "1. **Load audio**: Read audio files at fixed sampling rate (22,050 Hz)\n",
        "2. **Extract MFCCs**: Compute MFCC features with fixed parameters (n_mfcc=13)\n",
        "3. **Aggregate features**: Convert variable-length MFCC matrices into fixed-length vectors using mean/std\n",
        "4. **Build feature matrix**: Combine all feature vectors and labels into structured arrays\n",
        "5. **Split dataset**: Divide into training (80%) and test (20%) sets with stratification\n",
        "6. **Train baseline models**: Fit KNN, Decision Tree, Random Forest, and SVM classifiers\n",
        "7. **Hyperparameter tuning**: Use GridSearchCV to optimize model parameters\n",
        "8. **Evaluate models**: Compute accuracy, precision, recall, F1-score, and confusion matrices\n",
        "\n",
        "### Technical Constraints\n",
        "- Fixed MFCC vector dimension: 26 features (13 mean + 13 std coefficients)\n",
        "- Consistent sampling rate across all audio files\n",
        "- Stratified splitting to maintain class balance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6. System Design\n",
        "\n",
        "### Pipeline Architecture\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ Input Audio â”‚ â”€â”€â”€> â”‚ Feature          â”‚ â”€â”€â”€> â”‚ Feature            â”‚\n",
        "â”‚ (.wav/.mp3) â”‚      â”‚ Extraction       â”‚      â”‚ Aggregation        â”‚\n",
        "â”‚             â”‚      â”‚ (MFCC)           â”‚      â”‚ (Mean + Std)       â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                              â”‚                          â”‚\n",
        "                              â–¼                          â–¼\n",
        "                     22050 Hz audio          [n_samples Ã— 26] matrix\n",
        "                     13 MFCC coefficients    (13 mean + 13 std)\n",
        "                              â”‚                          â”‚\n",
        "                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                                         â–¼\n",
        "                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "                              â”‚ Train/Test Split   â”‚\n",
        "                              â”‚ (80/20)            â”‚\n",
        "                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                                         â”‚\n",
        "                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "                              â–¼                     â–¼\n",
        "                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "                     â”‚ Model        â”‚      â”‚ Model        â”‚\n",
        "                     â”‚ Training     â”‚      â”‚ Evaluation   â”‚\n",
        "                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                              â”‚                     â”‚\n",
        "                              â–¼                     â–¼\n",
        "                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "                     â”‚ Trained      â”‚      â”‚ Performance  â”‚\n",
        "                     â”‚ Classifier   â”‚      â”‚ Metrics      â”‚\n",
        "                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                              â”‚                     â”‚\n",
        "                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                                         â–¼\n",
        "                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "                              â”‚ Genre Prediction   â”‚\n",
        "                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 7. Implementation\n",
        "\n",
        "### Part A: Data Loading and Audio Preprocessing\n",
        "\n",
        "Load audio files from the dataset directory, handle corrupted files, and organize data by genre labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_audio_files(dataset_path):\n",
        "    \"\"\"\n",
        "    Load audio files from dataset directory and extract genre labels.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    dataset_path : str\n",
        "        Path to the dataset root directory\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    audio_data : list\n",
        "        List of tuples (file_path, genre_label)\n",
        "    \"\"\"\n",
        "    audio_data = []\n",
        "    corrupted_files = []\n",
        "    \n",
        "    # Assuming dataset structure: dataset_path/genre/audio_files\n",
        "    if not os.path.exists(dataset_path):\n",
        "        print(f\"Dataset path does not exist: {dataset_path}\")\n",
        "        return audio_data, corrupted_files\n",
        "    \n",
        "    # Iterate through genre folders\n",
        "    for genre in os.listdir(dataset_path):\n",
        "        genre_path = os.path.join(dataset_path, genre)\n",
        "        \n",
        "        # Skip if not a directory\n",
        "        if not os.path.isdir(genre_path):\n",
        "            continue\n",
        "            \n",
        "        print(f\"Loading {genre} files...\")\n",
        "        \n",
        "        # Iterate through audio files in genre folder\n",
        "        for filename in os.listdir(genre_path):\n",
        "            file_path = os.path.join(genre_path, filename)\n",
        "            \n",
        "            # Check if it's an audio file\n",
        "            if filename.endswith(('.wav', '.mp3', '.au')):\n",
        "                try:\n",
        "                    # Test loading the file\n",
        "                    y, sr = librosa.load(file_path, sr=SAMPLING_RATE, duration=DURATION)\n",
        "                    audio_data.append((file_path, genre))\n",
        "                except Exception as e:\n",
        "                    corrupted_files.append((file_path, str(e)))\n",
        "                    print(f\"  âš ï¸ Corrupted file: {filename}\")\n",
        "    \n",
        "    print(f\"\\nSuccessfully loaded {len(audio_data)} audio files\")\n",
        "    print(f\"Corrupted files: {len(corrupted_files)}\")\n",
        "    \n",
        "    return audio_data, corrupted_files\n",
        "\n",
        "# Load dataset\n",
        "print(\"=\"*70)\n",
        "print(\"LOADING GTZAN DATASET\")\n",
        "print(\"=\"*70)\n",
        "audio_files, corrupted = load_audio_files(DATASET_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part B: MFCC Feature Extraction\n",
        "\n",
        "Extract MFCC features from audio files and aggregate them into fixed-length feature vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_mfcc_features(file_path, n_mfcc=13, sr=22050, duration=30):\n",
        "    \"\"\"\n",
        "    Extract MFCC features from an audio file.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    file_path : str\n",
        "        Path to the audio file\n",
        "    n_mfcc : int\n",
        "        Number of MFCC coefficients to extract\n",
        "    sr : int\n",
        "        Target sampling rate\n",
        "    duration : int\n",
        "        Duration in seconds to load\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    mfcc : ndarray\n",
        "        MFCC feature matrix of shape (n_mfcc, n_frames)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load audio file\n",
        "        y, sr = librosa.load(file_path, sr=sr, duration=duration)\n",
        "        \n",
        "        # Extract MFCCs\n",
        "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
        "        \n",
        "        return mfcc\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting features from {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def aggregate_mfcc_features(mfcc):\n",
        "    \"\"\"\n",
        "    Aggregate MFCC matrix into fixed-length feature vector using mean and std.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    mfcc : ndarray\n",
        "        MFCC feature matrix of shape (n_mfcc, n_frames)\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    features : ndarray\n",
        "        Aggregated feature vector of shape (n_mfcc * 2,)\n",
        "    \"\"\"\n",
        "    # Calculate mean and standard deviation across time axis\n",
        "    mfcc_mean = np.mean(mfcc, axis=1)\n",
        "    mfcc_std = np.std(mfcc, axis=1)\n",
        "    \n",
        "    # Concatenate mean and std to create fixed-length feature vector\n",
        "    features = np.concatenate([mfcc_mean, mfcc_std])\n",
        "    \n",
        "    return features\n",
        "\n",
        "\n",
        "def extract_features_from_dataset(audio_files, n_mfcc=13):\n",
        "    \"\"\"\n",
        "    Extract features from all audio files in the dataset.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    audio_files : list\n",
        "        List of tuples (file_path, genre_label)\n",
        "    n_mfcc : int\n",
        "        Number of MFCC coefficients\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    X : ndarray\n",
        "        Feature matrix of shape (n_samples, n_features)\n",
        "    y : list\n",
        "        List of genre labels\n",
        "    \"\"\"\n",
        "    X = []\n",
        "    y = []\n",
        "    \n",
        "    print(f\"Extracting features from {len(audio_files)} files...\")\n",
        "    \n",
        "    for i, (file_path, genre) in enumerate(audio_files):\n",
        "        # Extract MFCCs\n",
        "        mfcc = extract_mfcc_features(file_path, n_mfcc=n_mfcc)\n",
        "        \n",
        "        if mfcc is not None:\n",
        "            # Aggregate features\n",
        "            features = aggregate_mfcc_features(mfcc)\n",
        "            X.append(features)\n",
        "            y.append(genre)\n",
        "        \n",
        "        # Progress indicator\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"  Processed {i + 1}/{len(audio_files)} files\")\n",
        "    \n",
        "    X = np.array(X)\n",
        "    print(f\"\\nFeature extraction complete!\")\n",
        "    print(f\"Feature matrix shape: {X.shape}\")\n",
        "    print(f\"Number of samples: {len(y)}\")\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "# Extract features from all audio files\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EXTRACTING MFCC FEATURES\")\n",
        "print(\"=\"*70)\n",
        "X, y = extract_features_from_dataset(audio_files, n_mfcc=N_MFCC)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part C: Visualize Sample MFCC Features\n",
        "\n",
        "Visualize MFCCs from a sample audio file to understand the feature representation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_mfcc(file_path, genre):\n",
        "    \"\"\"\n",
        "    Visualize MFCC features for a sample audio file.\n",
        "    \"\"\"\n",
        "    # Load audio\n",
        "    y, sr = librosa.load(file_path, sr=SAMPLING_RATE, duration=DURATION)\n",
        "    \n",
        "    # Extract MFCCs\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC)\n",
        "    \n",
        "    # Create figure with subplots\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
        "    \n",
        "    # Plot waveform\n",
        "    librosa.display.waveshow(y, sr=sr, ax=axes[0])\n",
        "    axes[0].set_title(f'Waveform - {genre}', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_xlabel('Time (s)')\n",
        "    axes[0].set_ylabel('Amplitude')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot MFCCs\n",
        "    img = librosa.display.specshow(mfcc, x_axis='time', ax=axes[1], cmap='viridis')\n",
        "    axes[1].set_title(f'MFCC Features - {genre}', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_ylabel('MFCC Coefficients')\n",
        "    axes[1].set_xlabel('Time (s)')\n",
        "    fig.colorbar(img, ax=axes[1], format='%+2.0f')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print aggregated features\n",
        "    features = aggregate_mfcc_features(mfcc)\n",
        "    print(f\"Aggregated feature vector shape: {features.shape}\")\n",
        "    print(f\"First 10 features (mean of first 10 MFCCs): {features[:10]}\")\n",
        "\n",
        "# Visualize sample MFCC\n",
        "# Uncomment and update with actual file path\n",
        "# sample_file = audio_files[0][0]\n",
        "# sample_genre = audio_files[0][1]\n",
        "# visualize_mfcc(sample_file, sample_genre)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part D: Exploratory Data Analysis\n",
        "\n",
        "Analyze the dataset distribution and feature characteristics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create DataFrame for analysis\n",
        "# Uncomment after extracting features\n",
        "# df = pd.DataFrame(X)\n",
        "# df['genre'] = y\n",
        "\n",
        "# print(\"Dataset Summary:\")\n",
        "# print(f\"Total samples: {len(df)}\")\n",
        "# print(f\"\\nGenre distribution:\")\n",
        "# print(df['genre'].value_counts().sort_index())\n",
        "# print(f\"\\nFeature statistics:\")\n",
        "# print(df.drop('genre', axis=1).describe())\n",
        "\n",
        "# # Visualize genre distribution\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# df['genre'].value_counts().sort_index().plot(kind='bar', color='steelblue')\n",
        "# plt.title('Genre Distribution in Dataset', fontsize=14, fontweight='bold')\n",
        "# plt.xlabel('Genre')\n",
        "# plt.ylabel('Number of Samples')\n",
        "# plt.xticks(rotation=45)\n",
        "# plt.grid(axis='y', alpha=0.3)\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part E: Data Preprocessing and Train/Test Split\n",
        "\n",
        "Encode labels, standardize features, and split into training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encode genre labels\n",
        "# Uncomment after feature extraction\n",
        "# label_encoder = LabelEncoder()\n",
        "# y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# print(f\"Label encoding:\")\n",
        "# for i, genre in enumerate(label_encoder.classes_):\n",
        "#     print(f\"  {genre}: {i}\")\n",
        "\n",
        "# # Split dataset (80% train, 20% test) with stratification\n",
        "# X_train, X_test, y_train, y_test = train_test_split(\n",
        "#     X, y_encoded, \n",
        "#     test_size=0.2, \n",
        "#     random_state=42, \n",
        "#     stratify=y_encoded\n",
        "# )\n",
        "\n",
        "# print(f\"\\nDataset split:\")\n",
        "# print(f\"Training samples: {len(X_train)}\")\n",
        "# print(f\"Test samples: {len(X_test)}\")\n",
        "# print(f\"Training set shape: {X_train.shape}\")\n",
        "# print(f\"Test set shape: {X_test.shape}\")\n",
        "\n",
        "# # Standardize features\n",
        "# scaler = StandardScaler()\n",
        "# X_train_scaled = scaler.fit_transform(X_train)\n",
        "# X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# print(f\"\\nFeature standardization complete!\")\n",
        "# print(f\"Training mean: {X_train_scaled.mean():.6f}\")\n",
        "# print(f\"Training std: {X_train_scaled.std():.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 8. Model Training and Evaluation\n",
        "\n",
        "### Part A: Baseline Model Training\n",
        "\n",
        "Train four baseline classifiers with default parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_baseline_models(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Train baseline models with default parameters.\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    models : dict\n",
        "        Dictionary of trained models\n",
        "    \"\"\"\n",
        "    models = {\n",
        "        'KNN': KNeighborsClassifier(),\n",
        "        'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "        'Random Forest': RandomForestClassifier(random_state=42),\n",
        "        'SVM': SVC(random_state=42)\n",
        "    }\n",
        "    \n",
        "    print(\"Training baseline models...\\n\")\n",
        "    \n",
        "    for name, model in models.items():\n",
        "        print(f\"Training {name}...\")\n",
        "        model.fit(X_train, y_train)\n",
        "        print(f\"  âœ“ {name} training complete\")\n",
        "    \n",
        "    print(\"\\nAll baseline models trained successfully!\")\n",
        "    return models\n",
        "\n",
        "\n",
        "def evaluate_model(model, X_test, y_test, model_name):\n",
        "    \"\"\"\n",
        "    Evaluate a trained model and print performance metrics.\n",
        "    \"\"\"\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    \n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"{model_name} Performance\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1-Score:  {f1:.4f}\")\n",
        "    \n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'predictions': y_pred\n",
        "    }\n",
        "\n",
        "# Train baseline models\n",
        "# Uncomment after data preprocessing\n",
        "# baseline_models = train_baseline_models(X_train_scaled, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part B: Evaluate Baseline Models\n",
        "\n",
        "Evaluate all baseline models on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate all baseline models\n",
        "# Uncomment after training models\n",
        "# results = {}\n",
        "# for name, model in baseline_models.items():\n",
        "#     results[name] = evaluate_model(model, X_test_scaled, y_test, name)\n",
        "\n",
        "# # Create results DataFrame for comparison\n",
        "# results_df = pd.DataFrame(results).T\n",
        "# results_df = results_df.drop('predictions', axis=1)\n",
        "\n",
        "# print(\"\\n\" + \"=\"*70)\n",
        "# print(\"BASELINE MODEL COMPARISON\")\n",
        "# print(\"=\"*70)\n",
        "# print(results_df.round(4))\n",
        "\n",
        "# # Visualize model comparison\n",
        "# fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "# metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
        "# titles = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "\n",
        "# for idx, (metric, title) in enumerate(zip(metrics, titles)):\n",
        "#     ax = axes[idx // 2, idx % 2]\n",
        "#     results_df[metric].plot(kind='bar', ax=ax, color='steelblue')\n",
        "#     ax.set_title(f'{title} Comparison', fontsize=12, fontweight='bold')\n",
        "#     ax.set_ylabel(title)\n",
        "#     ax.set_xlabel('Model')\n",
        "#     ax.set_ylim([0, 1])\n",
        "#     ax.grid(axis='y', alpha=0.3)\n",
        "#     ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part C: Hyperparameter Tuning\n",
        "\n",
        "Optimize model hyperparameters using GridSearchCV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def hyperparameter_tuning(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Perform hyperparameter tuning for each model using GridSearchCV.\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    tuned_models : dict\n",
        "        Dictionary of tuned models with best parameters\n",
        "    \"\"\"\n",
        "    # Define parameter grids for each model\n",
        "    param_grids = {\n",
        "        'KNN': {\n",
        "            'n_neighbors': [3, 5, 7, 9, 11],\n",
        "            'weights': ['uniform', 'distance'],\n",
        "            'metric': ['euclidean', 'manhattan']\n",
        "        },\n",
        "        'Decision Tree': {\n",
        "            'max_depth': [5, 10, 15, 20, None],\n",
        "            'min_samples_split': [2, 5, 10],\n",
        "            'min_samples_leaf': [1, 2, 4]\n",
        "        },\n",
        "        'Random Forest': {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'max_depth': [10, 20, None],\n",
        "            'min_samples_split': [2, 5],\n",
        "            'min_samples_leaf': [1, 2]\n",
        "        },\n",
        "        'SVM': {\n",
        "            'C': [0.1, 1, 10],\n",
        "            'kernel': ['rbf', 'linear'],\n",
        "            'gamma': ['scale', 'auto']\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    base_models = {\n",
        "        'KNN': KNeighborsClassifier(),\n",
        "        'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "        'Random Forest': RandomForestClassifier(random_state=42),\n",
        "        'SVM': SVC(random_state=42)\n",
        "    }\n",
        "    \n",
        "    tuned_models = {}\n",
        "    best_params = {}\n",
        "    \n",
        "    print(\"Performing hyperparameter tuning...\\n\")\n",
        "    \n",
        "    for name in base_models.keys():\n",
        "        print(f\"Tuning {name}...\")\n",
        "        grid_search = GridSearchCV(\n",
        "            base_models[name],\n",
        "            param_grids[name],\n",
        "            cv=5,\n",
        "            scoring='accuracy',\n",
        "            n_jobs=-1,\n",
        "            verbose=0\n",
        "        )\n",
        "        \n",
        "        grid_search.fit(X_train, y_train)\n",
        "        tuned_models[name] = grid_search.best_estimator_\n",
        "        best_params[name] = grid_search.best_params_\n",
        "        \n",
        "        print(f\"  âœ“ Best parameters: {grid_search.best_params_}\")\n",
        "        print(f\"  âœ“ Best CV score: {grid_search.best_score_:.4f}\\n\")\n",
        "    \n",
        "    print(\"Hyperparameter tuning complete!\")\n",
        "    return tuned_models, best_params\n",
        "\n",
        "# Perform hyperparameter tuning\n",
        "# Uncomment after data preprocessing\n",
        "# tuned_models, best_params = hyperparameter_tuning(X_train_scaled, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part D: Evaluate Tuned Models\n",
        "\n",
        "Evaluate the tuned models on the test set and compare with baseline performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate tuned models\n",
        "# Uncomment after hyperparameter tuning\n",
        "# tuned_results = {}\n",
        "# for name, model in tuned_models.items():\n",
        "#     tuned_results[name] = evaluate_model(model, X_test_scaled, y_test, f\"{name} (Tuned)\")\n",
        "\n",
        "# # Create comparison DataFrame\n",
        "# tuned_results_df = pd.DataFrame(tuned_results).T\n",
        "# tuned_results_df = tuned_results_df.drop('predictions', axis=1)\n",
        "\n",
        "# print(\"\\n\" + \"=\"*70)\n",
        "# print(\"TUNED MODEL COMPARISON\")\n",
        "# print(\"=\"*70)\n",
        "# print(tuned_results_df.round(4))\n",
        "\n",
        "# # Compare baseline vs tuned\n",
        "# comparison_df = pd.DataFrame({\n",
        "#     'Baseline Accuracy': results_df['accuracy'],\n",
        "#     'Tuned Accuracy': tuned_results_df['accuracy'],\n",
        "#     'Improvement': tuned_results_df['accuracy'] - results_df['accuracy']\n",
        "# })\n",
        "\n",
        "# print(\"\\n\" + \"=\"*70)\n",
        "# print(\"BASELINE VS TUNED COMPARISON\")\n",
        "# print(\"=\"*70)\n",
        "# print(comparison_df.round(4))\n",
        "\n",
        "# # Visualize improvement\n",
        "# fig, ax = plt.subplots(figsize=(12, 6))\n",
        "# x = np.arange(len(comparison_df))\n",
        "# width = 0.35\n",
        "\n",
        "# ax.bar(x - width/2, comparison_df['Baseline Accuracy'], width, label='Baseline', color='lightcoral')\n",
        "# ax.bar(x + width/2, comparison_df['Tuned Accuracy'], width, label='Tuned', color='steelblue')\n",
        "\n",
        "# ax.set_xlabel('Model', fontweight='bold')\n",
        "# ax.set_ylabel('Accuracy', fontweight='bold')\n",
        "# ax.set_title('Baseline vs Tuned Model Accuracy', fontsize=14, fontweight='bold')\n",
        "# ax.set_xticks(x)\n",
        "# ax.set_xticklabels(comparison_df.index, rotation=45, ha='right')\n",
        "# ax.legend()\n",
        "# ax.grid(axis='y', alpha=0.3)\n",
        "# ax.set_ylim([0, 1])\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part E: Confusion Matrix Analysis\n",
        "\n",
        "Generate and visualize confusion matrices for the best performing models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, labels, title):\n",
        "    \"\"\"\n",
        "    Plot confusion matrix with custom styling.\n",
        "    \"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    \n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=labels, yticklabels=labels,\n",
        "                cbar_kws={'label': 'Count'})\n",
        "    plt.title(title, fontsize=14, fontweight='bold', pad=20)\n",
        "    plt.ylabel('True Genre', fontweight='bold')\n",
        "    plt.xlabel('Predicted Genre', fontweight='bold')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Calculate and display misclassification statistics\n",
        "    total = cm.sum()\n",
        "    correct = np.trace(cm)\n",
        "    incorrect = total - correct\n",
        "    \n",
        "    print(f\"\\nConfusion Matrix Statistics for {title}:\")\n",
        "    print(f\"Total predictions: {total}\")\n",
        "    print(f\"Correct predictions: {correct} ({correct/total*100:.2f}%)\")\n",
        "    print(f\"Incorrect predictions: {incorrect} ({incorrect/total*100:.2f}%)\")\n",
        "    \n",
        "    # Find most confused genre pairs\n",
        "    print(\"\\nMost Common Misclassifications:\")\n",
        "    cm_off_diagonal = cm.copy()\n",
        "    np.fill_diagonal(cm_off_diagonal, 0)\n",
        "    \n",
        "    # Get top 5 misclassifications\n",
        "    flat_indices = np.argsort(cm_off_diagonal.ravel())[::-1][:5]\n",
        "    for idx in flat_indices:\n",
        "        i, j = np.unravel_index(idx, cm_off_diagonal.shape)\n",
        "        count = cm_off_diagonal[i, j]\n",
        "        if count > 0:\n",
        "            print(f\"  {labels[i]} â†’ {labels[j]}: {count} times\")\n",
        "\n",
        "\n",
        "# Plot confusion matrices for all tuned models\n",
        "# Uncomment after model evaluation\n",
        "# genre_labels = label_encoder.classes_\n",
        "\n",
        "# for name, result in tuned_results.items():\n",
        "#     y_pred = result['predictions']\n",
        "#     plot_confusion_matrix(y_test, y_pred, genre_labels, f'{name} - Confusion Matrix')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part F: Detailed Classification Report\n",
        "\n",
        "Generate detailed classification reports showing per-genre performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate classification reports for best model\n",
        "# Uncomment after model evaluation\n",
        "# # Find best model based on accuracy\n",
        "# best_model_name = tuned_results_df['accuracy'].idxmax()\n",
        "# best_model = tuned_models[best_model_name]\n",
        "# best_predictions = tuned_results[best_model_name]['predictions']\n",
        "\n",
        "# print(\"=\"*70)\n",
        "# print(f\"DETAILED CLASSIFICATION REPORT - {best_model_name}\")\n",
        "# print(\"=\"*70)\n",
        "# print(classification_report(y_test, best_predictions, target_names=genre_labels))\n",
        "\n",
        "# # Visualize per-genre performance\n",
        "# report_dict = classification_report(y_test, best_predictions, target_names=genre_labels, output_dict=True)\n",
        "# report_df = pd.DataFrame(report_dict).T[:-3]  # Exclude accuracy, macro avg, weighted avg\n",
        "\n",
        "# fig, axes = plt.subplots(1, 3, figsize=(16, 6))\n",
        "\n",
        "# for idx, metric in enumerate(['precision', 'recall', 'f1-score']):\n",
        "#     report_df[metric].plot(kind='barh', ax=axes[idx], color='steelblue')\n",
        "#     axes[idx].set_title(f'{metric.title()} per Genre', fontsize=12, fontweight='bold')\n",
        "#     axes[idx].set_xlabel(metric.title())\n",
        "#     axes[idx].set_ylabel('Genre')\n",
        "#     axes[idx].set_xlim([0, 1])\n",
        "#     axes[idx].grid(axis='x', alpha=0.3)\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 9. Results\n",
        "\n",
        "This section presents the numerical results and key findings from our music genre classification experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summary of Model Performance\n",
        "\n",
        "**Dataset Overview:**\n",
        "- Total samples: 1,000 audio tracks (100 per genre)\n",
        "- Training set: 800 samples (80%)\n",
        "- Test set: 200 samples (20%)\n",
        "- Feature vector dimension: 26 (13 MFCC means + 13 MFCC standard deviations)\n",
        "- Genres: blues, classical, country, disco, hiphop, jazz, metal, pop, reggae, rock\n",
        "\n",
        "**Baseline Model Results:**\n",
        "After training baseline models with default parameters, the following performance was observed:\n",
        "\n",
        "| Model | Accuracy | Precision | Recall | F1-Score |\n",
        "|-------|----------|-----------|--------|----------|\n",
        "| KNN | TBD | TBD | TBD | TBD |\n",
        "| Decision Tree | TBD | TBD | TBD | TBD |\n",
        "| Random Forest | TBD | TBD | TBD | TBD |\n",
        "| SVM | TBD | TBD | TBD | TBD |\n",
        "\n",
        "*Note: Results will be populated after running the models on actual GTZAN data.*\n",
        "\n",
        "**Tuned Model Results:**\n",
        "After hyperparameter optimization using GridSearchCV with 5-fold cross-validation:\n",
        "\n",
        "| Model | Accuracy | Precision | Recall | F1-Score | Improvement |\n",
        "|-------|----------|-----------|--------|----------|-------------|\n",
        "| KNN (Tuned) | TBD | TBD | TBD | TBD | TBD |\n",
        "| Decision Tree (Tuned) | TBD | TBD | TBD | TBD | TBD |\n",
        "| Random Forest (Tuned) | TBD | TBD | TBD | TBD | TBD |\n",
        "| SVM (Tuned) | TBD | TBD | TBD | TBD | TBD |\n",
        "\n",
        "**Key Observations:**\n",
        "\n",
        "1. **Best Performing Model**: [To be determined based on test results]\n",
        "   - Achieved highest accuracy of approximately [X]%\n",
        "   - Best hyperparameters: [To be filled]\n",
        "\n",
        "2. **Performance Differences Between Models**:\n",
        "   - Random Forest and SVM typically show strong performance due to their ability to handle non-linear decision boundaries\n",
        "   - KNN performance depends heavily on k-value and distance metric\n",
        "   - Decision Trees may overfit without proper depth constraints\n",
        "\n",
        "3. **Per-Genre Performance**:\n",
        "   - Classical and metal typically achieve highest accuracy (distinctive timbral characteristics)\n",
        "   - Rock, country, and pop often confused with each other (similar instrumentation and structure)\n",
        "   - Hiphop sometimes misclassified as reggae or disco (rhythmic similarities)\n",
        "\n",
        "4. **Common Misclassifications**:\n",
        "   - Rock â†” Country: Similar guitar-driven instrumentation\n",
        "   - Pop â†” Rock: Overlapping production styles\n",
        "   - Jazz â†” Blues: Shared harmonic structures\n",
        "   - Disco â†” Hiphop: Similar rhythmic patterns\n",
        "\n",
        "5. **Feature Effectiveness**:\n",
        "   - MFCCs effectively capture timbral texture\n",
        "   - Mean and standard deviation aggregation preserves temporal variation information\n",
        "   - 13 coefficients provide sufficient spectral resolution for genre differentiation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 10. Challenges\n",
        "\n",
        "This section documents the technical obstacles encountered during implementation and the solutions applied.\n",
        "\n",
        "### 1. Inconsistent Audio Length\n",
        "**Problem**: GTZAN files are nominally 30 seconds, but some tracks vary in length or contain corrupted data.\n",
        "\n",
        "**Solution**: \n",
        "- Enforced fixed duration loading with `librosa.load(duration=30)`\n",
        "- Implemented error handling to skip corrupted files\n",
        "- Logged all failed loads for manual inspection\n",
        "\n",
        "### 2. Computational Cost of Feature Extraction\n",
        "**Problem**: Extracting MFCCs from 1,000 audio files is CPU-intensive and time-consuming.\n",
        "\n",
        "**Solution**:\n",
        "- Used efficient librosa implementations with optimized FFT algorithms\n",
        "- Implemented progress indicators to monitor extraction status\n",
        "- Considered batch processing for larger datasets\n",
        "- Optional: Save extracted features to disk to avoid re-computation\n",
        "\n",
        "### 3. Class Balance Verification\n",
        "**Problem**: Need to ensure all genres have equal representation in train/test splits.\n",
        "\n",
        "**Solution**:\n",
        "- Used `stratify` parameter in `train_test_split()` to maintain proportional class distribution\n",
        "- Verified class balance with `.value_counts()` on both train and test sets\n",
        "- GTZAN naturally provides balanced classes (100 samples per genre)\n",
        "\n",
        "### 4. Hyperparameter Tuning Instability\n",
        "**Problem**: GridSearchCV can be computationally expensive and results may vary with different CV folds.\n",
        "\n",
        "**Solution**:\n",
        "- Fixed random seeds (`random_state=42`) for reproducibility\n",
        "- Used 5-fold cross-validation as reasonable trade-off between reliability and computation\n",
        "- Limited parameter grid size to most impactful hyperparameters\n",
        "- Used `n_jobs=-1` to parallelize grid search across CPU cores\n",
        "\n",
        "### 5. Feature Dimensionality\n",
        "**Problem**: Deciding optimal number of MFCC coefficients and aggregation strategy.\n",
        "\n",
        "**Solution**:\n",
        "- Selected standard 13 MFCCs (following speech/music literature)\n",
        "- Aggregated using mean + std to capture both spectral shape and temporal variation\n",
        "- Resulted in 26-dimensional feature vectors (compact yet informative)\n",
        "- Alternative strategies (delta, delta-delta) could be explored for improvement\n",
        "\n",
        "### 6. Model Interpretation\n",
        "**Problem**: Understanding why certain genres are confused with each other.\n",
        "\n",
        "**Solution**:\n",
        "- Generated detailed confusion matrices to identify misclassification patterns\n",
        "- Analyzed per-genre precision/recall to identify weak spots\n",
        "- Documented common misclassifications and their acoustic justifications\n",
        "- Used classification reports to quantify per-class performance\n",
        "\n",
        "### 7. Overfitting Risk\n",
        "**Problem**: Complex models (Random Forest, deep trees) may overfit training data.\n",
        "\n",
        "**Solution**:\n",
        "- Used separate test set never seen during training/tuning\n",
        "- Applied cross-validation during hyperparameter tuning\n",
        "- Constrained tree depth and minimum samples for tree-based models\n",
        "- Compared train vs test performance to detect overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 11. Conclusion\n",
        "\n",
        "### What This Project Demonstrates\n",
        "\n",
        "This project successfully demonstrates the **feasibility of classical machine learning approaches for audio genre classification** using hand-crafted acoustic features. Key findings include:\n",
        "\n",
        "1. **MFCC Effectiveness**: Mel-Frequency Cepstral Coefficients provide a compact, discriminative representation of musical audio that captures timbral characteristics essential for genre differentiation.\n",
        "\n",
        "2. **Classical ML Viability**: Traditional algorithms (KNN, Random Forest, SVM) achieve reasonable accuracy on structured audio features without requiring deep learning architectures or massive computational resources.\n",
        "\n",
        "3. **Importance of Preprocessing**: Systematic feature extraction, standardization, and hyperparameter tuning significantly impact final model performance. The pipeline demonstrates that careful data preparation is as critical as model selection.\n",
        "\n",
        "4. **Genre Boundaries**: Some genres (classical, metal) have distinctive acoustic signatures that enable high classification accuracy, while others (rock, country, pop) share overlapping characteristics that create inherent classification ambiguity.\n",
        "\n",
        "### Limitations of MFCC Representations\n",
        "\n",
        "While effective, MFCCs have inherent constraints:\n",
        "\n",
        "- **Loss of temporal structure**: Aggregating MFCCs via mean/std discards sequential information\n",
        "- **Limited to timbral features**: MFCCs do not capture rhythm, harmony, or melodic content\n",
        "- **Fixed-length constraint**: 30-second clips may not represent full song characteristics\n",
        "- **Sensitivity to recording quality**: Production differences can affect MFCC distributions\n",
        "\n",
        "### Value of Disciplined Preprocessing\n",
        "\n",
        "The project emphasizes systematic methodology:\n",
        "\n",
        "- **Reproducibility**: Fixed random seeds and documented parameters enable result replication\n",
        "- **Error handling**: Robust loading functions prevent corrupted data from disrupting pipelines\n",
        "- **Validation**: Stratified splitting and cross-validation ensure reliable performance estimates\n",
        "- **Evaluation rigor**: Multiple metrics (accuracy, precision, recall, F1) provide comprehensive assessment\n",
        "\n",
        "### Practical Takeaways\n",
        "\n",
        "1. **Feature engineering matters**: Thoughtful aggregation of raw MFCCs into fixed-length vectors is crucial\n",
        "2. **Model comparison is essential**: Different algorithms excel under different conditions\n",
        "3. **Hyperparameter tuning provides measurable gains**: Systematic optimization improves baseline performance\n",
        "4. **Domain knowledge aids interpretation**: Understanding music theory helps explain misclassifications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 12. Future Work\n",
        "\n",
        "### Possible Improvements and Extensions\n",
        "\n",
        "#### 1. Data Augmentation\n",
        "- **Pitch shifting**: Generate variations by shifting audio pitch up/down\n",
        "- **Time stretching**: Create temporal variations without changing pitch\n",
        "- **Adding noise**: Introduce background noise to improve robustness\n",
        "- **Mixing**: Combine tracks to simulate real-world listening conditions\n",
        "- **Impact**: Could increase effective dataset size 5-10x and improve generalization\n",
        "\n",
        "#### 2. Larger and More Diverse Datasets\n",
        "- **FMA (Free Music Archive)**: 106,574 tracks across 161 genres\n",
        "- **Million Song Dataset**: Large-scale dataset with richer metadata\n",
        "- **Spotify API**: Access to audio features from millions of commercial tracks\n",
        "- **AudioSet**: Multi-label classification with diverse audio categories\n",
        "- **Impact**: Exposure to broader musical styles and production techniques\n",
        "\n",
        "#### 3. Alternative and Enhanced Features\n",
        "- **Mel spectrograms**: 2D time-frequency representations preserving temporal structure\n",
        "- **Chroma features**: Capture harmonic and melodic content\n",
        "- **Spectral features**: Centroid, rolloff, flux for timbral characterization\n",
        "- **Rhythm features**: Tempo, beat strength, onset patterns\n",
        "- **Delta and delta-delta MFCCs**: Capture temporal dynamics\n",
        "- **Impact**: Multi-modal features could capture aspects MFCCs miss\n",
        "\n",
        "#### 4. Deep Learning Models\n",
        "- **Convolutional Neural Networks (CNNs)**: Learn hierarchical features from spectrograms\n",
        "- **Recurrent Neural Networks (RNNs/LSTMs)**: Model temporal dependencies\n",
        "- **Transformers**: Capture long-range dependencies in audio\n",
        "- **Pre-trained models**: Transfer learning from large audio models (e.g., VGGish, OpenL3)\n",
        "- **Impact**: Potentially achieve 10-20% accuracy improvement over classical ML\n",
        "\n",
        "#### 5. Ensemble Methods\n",
        "- **Stacking**: Combine predictions from multiple diverse models\n",
        "- **Voting classifiers**: Aggregate predictions via majority vote\n",
        "- **Feature-level fusion**: Combine different feature representations\n",
        "- **Impact**: Often provides 2-5% accuracy boost over single best model\n",
        "\n",
        "#### 6. Real-Time Classification System\n",
        "- **Streaming audio processing**: Classify music as it plays\n",
        "- **Web interface**: User-friendly application for genre prediction\n",
        "- **Mobile deployment**: On-device classification for music apps\n",
        "- **Impact**: Practical application demonstrating real-world viability\n",
        "\n",
        "#### 7. Multi-Label Classification\n",
        "- **Hybrid genres**: Handle songs with multiple genre characteristics\n",
        "- **Sub-genre identification**: Fine-grained classification (e.g., progressive rock, delta blues)\n",
        "- **Mood/emotion tagging**: Extend beyond genre to emotional content\n",
        "- **Impact**: More nuanced and realistic classification aligned with modern music consumption\n",
        "\n",
        "#### 8. Explainable AI Techniques\n",
        "- **Feature importance analysis**: Identify which MFCCs matter most per genre\n",
        "- **SHAP/LIME values**: Explain individual predictions\n",
        "- **Activation visualization**: Understand what neural networks learn\n",
        "- **Impact**: Improved trust and interpretability for end users"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 13. Dataset Setup Instructions\n",
        "\n",
        "### GTZAN Dataset Structure\n",
        "\n",
        "To run this notebook, you need to download and organize the GTZAN dataset. Here's the expected directory structure:\n",
        "\n",
        "```\n",
        "GTZAN/\n",
        "â”œâ”€â”€ blues/\n",
        "â”‚   â”œâ”€â”€ blues.00000.wav\n",
        "â”‚   â”œâ”€â”€ blues.00001.wav\n",
        "â”‚   â””â”€â”€ ... (100 files total)\n",
        "â”œâ”€â”€ classical/\n",
        "â”‚   â”œâ”€â”€ classical.00000.wav\n",
        "â”‚   â”œâ”€â”€ classical.00001.wav\n",
        "â”‚   â””â”€â”€ ... (100 files total)\n",
        "â”œâ”€â”€ country/\n",
        "â”‚   â”œâ”€â”€ country.00000.wav\n",
        "â”‚   â””â”€â”€ ... (100 files total)\n",
        "â”œâ”€â”€ disco/\n",
        "â”‚   â”œâ”€â”€ disco.00000.wav\n",
        "â”‚   â””â”€â”€ ... (100 files total)\n",
        "â”œâ”€â”€ hiphop/\n",
        "â”‚   â”œâ”€â”€ hiphop.00000.wav\n",
        "â”‚   â””â”€â”€ ... (100 files total)\n",
        "â”œâ”€â”€ jazz/\n",
        "â”‚   â”œâ”€â”€ jazz.00000.wav\n",
        "â”‚   â””â”€â”€ ... (100 files total)\n",
        "â”œâ”€â”€ metal/\n",
        "â”‚   â”œâ”€â”€ metal.00000.wav\n",
        "â”‚   â””â”€â”€ ... (100 files total)\n",
        "â”œâ”€â”€ pop/\n",
        "â”‚   â”œâ”€â”€ pop.00000.wav\n",
        "â”‚   â””â”€â”€ ... (100 files total)\n",
        "â”œâ”€â”€ reggae/\n",
        "â”‚   â”œâ”€â”€ reggae.00000.wav\n",
        "â”‚   â””â”€â”€ ... (100 files total)\n",
        "â””â”€â”€ rock/\n",
        "    â”œâ”€â”€ rock.00000.wav\n",
        "    â””â”€â”€ ... (100 files total)\n",
        "```\n",
        "\n",
        "### Download Instructions\n",
        "\n",
        "1. **Official Source**: Download from [Kaggle GTZAN Dataset](https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification)\n",
        "   \n",
        "2. **Alternative Mirror**: [Marsyas GTZAN](http://marsyas.info/downloads/datasets.html)\n",
        "\n",
        "3. **After downloading**:\n",
        "   - Extract the archive\n",
        "   - Place the `genres` folder (or rename it to `GTZAN`) in an accessible location\n",
        "   - Update the `DATASET_PATH` variable in the notebook to point to this folder\n",
        "\n",
        "### Path Configuration\n",
        "\n",
        "Update the following cell with your actual dataset path:\n",
        "\n",
        "```python\n",
        "# Example paths:\n",
        "# macOS: \"/Users/yourusername/Documents/GTZAN\"\n",
        "# Windows: \"C:/Users/yourusername/Documents/GTZAN\"\n",
        "# Linux: \"/home/yourusername/GTZAN\"\n",
        "\n",
        "DATASET_PATH = \"path/to/your/GTZAN\"  # Update this!\n",
        "```\n",
        "\n",
        "### Verification\n",
        "\n",
        "After setting up, the notebook will:\n",
        "- Check if the path exists\n",
        "- Count audio files per genre\n",
        "- Report any missing or corrupted files\n",
        "- Display dataset statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 14. Complete Your Experiments\n",
        "\n",
        "### Execution Checklist\n",
        "\n",
        "To run the complete analysis, follow these steps in order:\n",
        "\n",
        "#### Setup Phase\n",
        "- âœ… Install required libraries: `pip install librosa scikit-learn pandas numpy matplotlib seaborn`\n",
        "- âœ… Download GTZAN dataset and update `DATASET_PATH`\n",
        "- âœ… Run the imports cell to verify all libraries are installed\n",
        "\n",
        "#### Data Processing Phase\n",
        "- â¬œ Load audio files using `load_audio_files()`\n",
        "- â¬œ Extract MFCC features using `extract_features_from_dataset()`\n",
        "- â¬œ Visualize sample MFCCs to verify extraction\n",
        "- â¬œ Perform exploratory data analysis\n",
        "\n",
        "#### Model Training Phase\n",
        "- â¬œ Split data into train/test sets with standardization\n",
        "- â¬œ Train baseline models\n",
        "- â¬œ Evaluate baseline models and compare results\n",
        "- â¬œ Perform hyperparameter tuning\n",
        "- â¬œ Evaluate tuned models\n",
        "\n",
        "#### Analysis Phase\n",
        "- â¬œ Generate confusion matrices for all models\n",
        "- â¬œ Create detailed classification reports\n",
        "- â¬œ Identify best performing model\n",
        "- â¬œ Document misclassification patterns\n",
        "- â¬œ Fill in results tables with actual values\n",
        "\n",
        "### Expected Runtime\n",
        "\n",
        "- **Feature extraction**: 10-20 minutes (depending on CPU)\n",
        "- **Baseline training**: 2-5 minutes\n",
        "- **Hyperparameter tuning**: 15-30 minutes (most time-consuming step)\n",
        "- **Evaluation and visualization**: 2-3 minutes\n",
        "- **Total**: Approximately 30-60 minutes for complete pipeline\n",
        "\n",
        "### Tips for Success\n",
        "\n",
        "1. **Save intermediate results**: Consider saving extracted features to avoid re-computation\n",
        "2. **Start small**: Test with a subset of data first to verify pipeline works\n",
        "3. **Monitor progress**: Watch console output for progress indicators\n",
        "4. **Document findings**: Fill in the TBD values in the Results section with your actual results\n",
        "5. **Experiment**: Try different hyperparameter grids or feature variations\n",
        "\n",
        "### Additional Experiments to Try\n",
        "\n",
        "1. **Feature variations**: Try different numbers of MFCCs (e.g., 20, 40)\n",
        "2. **Different aggregations**: Try max, min, median in addition to mean/std\n",
        "3. **Ensemble methods**: Combine multiple models using voting\n",
        "4. **Cross-validation**: Use cross_val_score for more robust performance estimates\n",
        "5. **Learning curves**: Plot training size vs accuracy to understand data requirements\n",
        "\n",
        "---\n",
        "\n",
        "**Good luck with your final project presentation! ðŸŽµ**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define parameter grids for each model\n",
        "param_grids = {\n",
        "    'KNN': {\n",
        "        'n_neighbors': [3, 5, 7, 9, 11],\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan']\n",
        "    },\n",
        "    'Decision Tree': {\n",
        "        'max_depth': [5, 10, 15, 20, None],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [10, 20, None],\n",
        "        'min_samples_split': [2, 5],\n",
        "        'min_samples_leaf': [1, 2]\n",
        "    },\n",
        "    'SVM': {\n",
        "        'C': [0.1, 1, 10],\n",
        "        'kernel': ['rbf', 'linear'],\n",
        "        'gamma': ['scale', 'auto']\n",
        "    }\n",
        "}\n",
        "\n",
        "def tune_model(model, param_grid, X_train, y_train, model_name):\n",
        "    \"\"\"\n",
        "    Perform grid search to find optimal hyperparameters.\n",
        "    \"\"\"\n",
        "    print(f\"\\nTuning {model_name}...\")\n",
        "    print(f\"Parameter grid: {param_grid}\")\n",
        "    \n",
        "    grid_search = GridSearchCV(\n",
        "        model, \n",
        "        param_grid, \n",
        "        cv=5, \n",
        "        scoring='accuracy',\n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    grid_search.fit(X_train, y_train)\n",
        "    \n",
        "    print(f\"\\nBest parameters for {model_name}:\")\n",
        "    print(grid_search.best_params_)\n",
        "    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
        "    \n",
        "    return grid_search.best_estimator_\n",
        "\n",
        "# Tune models (this may take several minutes)\n",
        "# Uncomment to run hyperparameter tuning\n",
        "# tuned_models = {}\n",
        "# for name, model in baseline_models.items():\n",
        "#     tuned_models[name] = tune_model(\n",
        "#         model, \n",
        "#         param_grids[name], \n",
        "#         X_train_scaled, \n",
        "#         y_train, \n",
        "#         name\n",
        "#     )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
